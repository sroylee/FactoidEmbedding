settings: ./settings.cnf
[data]
	path:./ins_fb-tw_core/
	train_test_paths:["fold_0"]#, "fold_1", "fold_2", "fold_3", "fold_4"]
	query_path:fold_0
	source_prefix:ins
	target_prefix:fb-tw
	source_col:0
	target_col:1
[predicate_name]
	concatenate:True
	preprocess:True
	method:tfidf
	screen_name_exist:True
[predicate_image]
	exist:False
	method:vgg16
	identical_threshold:15.
[dispy]
	ip:10.0.109.76
	port:51348
	remote_path:/home/weixie/data/remote
[cosine_embedding]
	pass:False
	n_gpu:4
	n_dim:1024
	n_iter:5
	learning_rate:1
	batch_size:32*1024
	partition_path:/home/weixie/data/part
[triplet_embedding]
	supervised:False
	bias:True
	learning_rate_f:0.05
	learning_rate_a:5.
	learning_rate_n:5.
	snapshot:False
	snapshot_gap:3
	n_iter:150
	warm_up_iter:0
	batch_size:128
	user_dim:1024
	nce_sampling:2048
[debug]
	flag:True
source_user_names 3897
source_screen_names 3897
target_user_names 4093
target_screen_names 4093
source_users 3897 target_users 4093
num_attr 7990
prefix in name2sim concat_name
In name2sim() method  tfidf
Maximum N-gram length 5
TF-IDF matrix builded with  0.01501912275950114 mins.
<class 'scipy.sparse.csr.csr_matrix'> (7990, 123060)
['!', '!!', '!!!', '!!!m', '!!!me', '!!m', '!!me', '!!mei', '!c', '!ca', '!cat', '!cath', '!d', '!da', '!dap', '!daph', '!h', '!ha', '!i', '!is', '!isy', '!isya', '!l', '!ls', '!ls0', '!ls0n', '!m', '!me', '!mei', '!meim', '!p', '!pe', '!pei', '!peiw', '!q', '!qu', '!qui', '!quin', '"', '""', '"""', '"""a', '"""al', '"""j', '"""jd', '""a', '""al', '""ale', '""j', '""jd']
2018-08-13 06:28:49 pycos - version 4.6.5 with epoll I/O notifier
2018-08-13 06:28:49 dispy - dispy client version: 4.8.7
2018-08-13 06:28:49 dispy - Storing fault recovery information in "_dispy_20180813062849"
250
ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok

ok


                           Node |  CPUs |    Jobs |    Sec/Job | Node Time Sec
------------------------------------------------------------------------------
 10.0.109.76 (dgxv2)            |    32 |      32 |      0.945 |        30.250

Total job time: 30.250 sec, wall time: 1.715 sec, speedup: 17.635

final blks 32
partition_size 1998
update_ops 16
op (0, 0) --> (1, 0)
op (0, 0) --> (2, 0)
op (0, 0) --> (3, 0)
op (1, 1) --> (0, 1)
op (1, 1) --> (2, 1)
op (1, 1) --> (3, 1)
op (2, 2) --> (0, 2)
op (2, 2) --> (1, 2)
op (2, 2) --> (3, 2)
op (3, 3) --> (0, 3)
op (3, 3) --> (1, 3)
op (3, 3) --> (2, 3)
0 0 0 5
0_0_0 one_round 7
que_len 7
0 1 0 5
0_1_0 one_round 6
que_len 6
0 2 0 5
0_2_0 one_round 8
que_len 8
0 3 0 5
0_3_0 one_round 6
que_len 6
1 0 0 5
1_0_0 one_round 6
que_len 6
1 1 0 5
1_1_0 one_round 7
que_len 7
1 2 0 5
1_2_0 one_round 7
que_len 7
1 3 0 5
1_3_0 one_round 7
que_len 7
2 0 0 5
2_0_0 one_round 7
que_len 7
2 1 0 5
2_1_0 one_round 7
que_len 7
2 2 0 5
2_2_0 one_round 10
que_len 10
2 3 0 5
2_3_0 one_round 7
que_len 7
3 0 0 5
3_0_0 one_round 6
que_len 6
3 1 0 5
3_1_0 one_round 6
que_len 6
3 2 0 5
3_2_0 one_round 7
que_len 7
3 3 0 5
3_3_0 one_round 7
que_len 7
<class 'numpy.ndarray'> (211877,)
<class 'numpy.ndarray'> (211877,)
<class 'numpy.ndarray'> (211877,)
l= 211877
0.22402
<class 'numpy.ndarray'> (187434,)
<class 'numpy.ndarray'> (187434,)
<class 'numpy.ndarray'> (187434,)
l= 187434
0.246283
<class 'numpy.ndarray'> (192001,)
<class 'numpy.ndarray'> (192001,)
<class 'numpy.ndarray'> (192001,)
l= 192001
0.245591
<class 'numpy.ndarray'> (185004,)
<class 'numpy.ndarray'> (185004,)
<class 'numpy.ndarray'> (185004,)
l= 185004
0.243502
<class 'numpy.ndarray'> (225873,)
<class 'numpy.ndarray'> (225873,)
<class 'numpy.ndarray'> (225873,)
l= 225873
0.242209
<class 'numpy.ndarray'> (223129,)
<class 'numpy.ndarray'> (223129,)
<class 'numpy.ndarray'> (223129,)
l= 223129
0.260623
<class 'numpy.ndarray'> (217153,)
<class 'numpy.ndarray'> (217153,)
<class 'numpy.ndarray'> (217153,)
l= 217153
0.24056
<class 'numpy.ndarray'> (226941,)
<class 'numpy.ndarray'> (226941,)
<class 'numpy.ndarray'> (226941,)
l= 226941
0.24046
<class 'numpy.ndarray'> (182105,)
<class 'numpy.ndarray'> (182105,)
<class 'numpy.ndarray'> (182105,)
l= 182105
0.243067
<class 'numpy.ndarray'> (194078,)
<class 'numpy.ndarray'> (194078,)
<class 'numpy.ndarray'> (194078,)
l= 194078
0.280508
<class 'numpy.ndarray'> (223853,)
<class 'numpy.ndarray'> (223853,)
<class 'numpy.ndarray'> (223853,)
l= 223853
0.242143
0.214485
0.23848
<class 'numpy.ndarray'> (233706,)
<class 'numpy.ndarray'> (233706,)
<class 'numpy.ndarray'> (233706,)
l= 233706
0.26359
0.235145
<class 'numpy.ndarray'> (215833,)
<class 'numpy.ndarray'> (215833,)
<class 'numpy.ndarray'> (215833,)
l= 215833
0.2308
0.234154
<class 'numpy.ndarray'> (202012,)
<class 'numpy.ndarray'> (202012,)
<class 'numpy.ndarray'> (202012,)
l= 202012
0.26932
0.222619
0.237069
0.225615
0.223481
0.242739
<class 'numpy.ndarray'> (308196,)
<class 'numpy.ndarray'> (308196,)
<class 'numpy.ndarray'> (308196,)
l= 308196
0.22317
0.265077
0.24953
0.254221
0.248434
0.244397
0.243641
0.276138
_ct 1
0.282389
0.249839
_ct 1
0.216947
0.211281
_ct 1
<class 'numpy.ndarray'> (218951,)
<class 'numpy.ndarray'> (218951,)
<class 'numpy.ndarray'> (218951,)
l= 218951
0.235437
0.247429
0.241276
0.238784
_ct 1
0.22709
0.270359
_ct 1
0.238252
0.27685
0.246816
0.250431
_ct 1
0.219793
0.277776
_ct 1
0.231057
0.237974
0.236422
0.23956
0.232826
0.215024
0.247505
0.239727
0.223977
0.235978
_ct 1
0.241852
0.260329
0.218874
3_0_0 for each mini-batch 0.106065869331 0.235541244348
0.234332
0.246511
_ct 1
0.265023
_ct 1
0.210748
0.218342
_ct 1
0.224613
0.229185
0.236855
_ct 1
0.22794
0.263494
0.233905
0.228611
0.221727
0.255232
0.225736
0_1_0 for each mini-batch 0.107919573784 0.238569147885
_ct 1
_ct 1
0.208542
0.210841
0.241372
0.255156
0.247567
3_1_0 for each mini-batch 0.10839676857 0.268232134481
0.227599
0_3_0 for each mini-batch 0.108846664429 0.238204819461
0.221196
1_2_0 for each mini-batch 0.0932398523603 0.233602819698
0.230945
0.237067
0.209788
0.239331
0.214094
0.224052
0.226553
0.225017
1_0_0 for each mini-batch 0.11003168424 0.235578852395
0.232134
0.207136
0.242739
0.215848
2_1_0 for each mini-batch 0.094489472253 0.233230901616
0.223977
2_3_0 for each mini-batch 0.0945105893271 0.234764177884
0.235266
2_0_0 for each mini-batch 0.0946458067213 0.252792958702
0.218532
0.208112
0.209336
0.200832
1_1_0 for each mini-batch 0.0951162406376 0.215692362615
0.259621
0_2_0 for each mini-batch 0.0833360850811 0.251650951803
0.218674
3_2_0 for each mini-batch 0.0949340207236 0.23395285649
_ct 1
0.262317
1_3_0 for each mini-batch 0.0952531610216 0.260133283479
0.200538
0_0_0 for each mini-batch 0.095838206155 0.212319755128
0.21642
0.210742
_ct 1
0.206375
0.201582
0.202888
3_3_0 for each mini-batch 0.0962021350861 0.215292570846
0.203893
0.208231
2_2_0 for each mini-batch 0.0679393529892 0.2117055282
fold_0
unmatched_uid 0
triplets 43214
./ins_fb-tw_core/ins2fb-tw_results_unsupervised_biased_d1024_i150_w0_ns2048_tfidf_s/unsupervised_user_embedding_result.npy loaded
0%0%0%0%0%0%0%0%1%1%1%1%1%1%1%1%2%2%2%2%2%2%2%2%3%3%3%3%3%3%3%3%4%4%4%4%4%4%4%4%5%5%5%5%5%5%5%5%5%6%6%6%6%6%6%6%6%7%7%7%7%7%7%7%7%8%8%8%8%8%8%8%8%9%9%9%9%9%9%9%9%10%10%10%10%10%10%10%10%10%11%11%11%11%11%11%11%11%12%12%12%12%12%12%12%12%13%13%13%13%13%13%13%13%14%14%14%14%14%14%14%14%15%15%15%15%15%15%15%15%15%16%16%16%16%16%16%16%16%17%17%17%17%17%17%17%17%18%18%18%18%18%18%18%18%19%19%19%19%19%19%19%19%20%20%20%20%20%20%20%20%21%21%21%21%21%21%21%21%21%22%22%22%22%22%22%22%22%23%23%23%23%23%23%23%23%24%24%24%24%24%24%24%24%25%25%25%25%25%25%25%25%26%26%26%26%26%26%26%26%26%27%27%27%27%27%27%27%27%28%28%28%28%28%28%28%28%29%29%29%29%29%29%29%29%30%30%30%30%30%30%30%30%31%31%31%31%31%31%31%31%31%32%32%32%32%32%32%32%32%33%33%33%33%33%33%33%33%34%34%34%34%34%34%34%34%35%35%35%35%35%35%35%35%36%36%36%36%36%36%36%36%36%37%37%37%37%37%37%37%37%38%38%38%38%38%38%38%38%39%39%39%39%39%39%39%39%40%40%40%40%40%40%40%40%41%41%41%41%41%41%41%41%42%42%42%42%42%42%42%42%42%43%43%43%43%43%43%43%43%44%44%44%44%44%44%44%44%45%45%45%45%45%45%45%45%46%46%46%46%46%46%46%46%47%47%47%47%47%47%47%47%47%48%48%48%48%48%48%48%48%49%49%49%49%49%49%49%49%50%50%50%50%50%50%50%50%51%51%51%51%51%51%51%51%52%52%52%52%52%52%52%52%52%53%53%53%53%53%53%53%53%54%54%54%54%54%54%54%54%55%55%55%55%55%55%55%55%56%56%56%56%56%56%56%56%57%57%57%57%57%57%57%57%57%58%58%58%58%58%58%58%58%59%59%59%59%59%59%59%59%60%60%60%60%60%60%60%60%61%61%61%61%61%61%61%61%62%62%62%62%62%62%62%62%63%63%63%63%63%63%63%63%63%64%64%64%64%64%64%64%64%65%65%65%65%65%65%65%65%66%66%66%66%66%66%66%66%67%67%67%67%67%67%67%67%68%68%68%68%68%68%68%68%68%69%69%69%69%69%69%69%69%70%70%70%70%70%70%70%70%71%71%71%71%71%71%71%71%72%72%72%72%72%72%72%72%73%73%73%73%73%73%73%73%73%74%74%74%74%74%74%74%74%75%75%75%75%75%75%75%75%76%76%76%76%76%76%76%76%77%77%77%77%77%77%77%77%78%78%78%78%78%78%78%78%78%79%79%79%79%79%79%79%79%80%80%80%80%80%80%80%80%81%81%81%81%81%81%81%81%82%82%82%82%82%82%82%82%83%83%83%83%83%83%83%83%84%84%84%84%84%84%84%84%84%85%85%85%85%85%85%85%85%86%86%86%86%86%86%86%86%87%87%87%87%87%87%87%87%88%88%88%88%88%88%88%88%89%89%89%89%89%89%89%89%89%90%90%90%90%90%90%90%90%91%91%91%91%91%91%91%91%92%92%92%92%92%92%92%92%93%93%93%93%93%93%93%93%94%94%94%94%94%94%94%94%94%95%95%95%95%95%95%95%95%96%96%96%96%96%96%96%96%97%97%97%97%97%97%97%97%98%98%98%98%98%98%98%98%99%99%99%99%99%99%99%99%100%1.6534185409545898 seconds used.
all_res 819
[[ 0.9047619   0.94261294  0.94871795  0.94993895  0.95115995  0.95360195
   0.95360195  0.95482295  0.95482295  0.95604396  0.95604396  0.95604396
   0.95726496  0.95726496  0.95726496  0.95848596  0.95848596  0.95848596
   0.95970696  0.95970696  0.96092796  0.96092796  0.96092796  0.96214896
   0.96214896  0.96214896  0.96214896  0.96214896  0.96336996  0.96336996
   0.92762563]]
